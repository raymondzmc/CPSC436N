{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyG4lTR4D7Lg"
   },
   "source": [
    "#  Assignment 4 (part 2)(CPSC 436N): LSTM-Based Part-Of-Speech (POS) Tagger\n",
    "\n",
    "In Part2 of this assignment, we are also using the text file called __cmpt-hw2-3.txt__ to train a bidirectional stacked LSTM-based neural sequence labeling model to predict the part of speech tags of unknown input sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFaEQAQgEIew"
   },
   "source": [
    "## 1. import libs and packages needed for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Cti1kgGqD9a4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEzE9RjwE0fI"
   },
   "source": [
    "## 2. Load the dataset (cmpt-hw2-3.txt)\n",
    "\n",
    "We will use the same dataset as in (Part1), namely **cmpt-hw2-3.txt**. Please first upload this text file from your local computer to Colab by following these instructions:\n",
    "\n",
    "https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "(Please note that the uploaded file will be deleted once the connection is terminated. So you need to upload the file everytime you connect to google colab.)\n",
    "\n",
    "<br>\n",
    "\n",
    "For convenience, we will load and save each line of the dataset as **a token list** and **a pos list**. For example, one line in our dataset:\n",
    "\n",
    "*There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n",
    "\n",
    "will be saved as:\n",
    "\n",
    "__token list:__ ['there', 'are', 'also', 'plant', 'and', 'gift', 'shops', '.']\n",
    "\n",
    "__pos list:__ ['EX', 'VBP', 'RB', 'NN', 'CC', 'NN', 'NNS', '.']\n",
    "\n",
    "(Please note that we convert each token into its lower case.)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Alternatively, you can run the entire Notebook on your local machine, where training takes around 2 minutes per epoch on a relatively modern machine (i.e. MacBook Pro).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n_ob5y-z7psi"
   },
   "outputs": [],
   "source": [
    "data_path = './cmpt-hw2-3.txt'\n",
    "\n",
    "dataset = [] #initialize dataset list.\n",
    "for line in open(data_path): #load data from the text file.\n",
    "    text_list = []; pos_list = [];\n",
    "    l = line.strip().split(' ') #split each line in the file by space.\n",
    "    for w in l:\n",
    "      w = w.split('_') #each token and its pos tag are connected by an underscore, here we split it by undescore.\n",
    "      text_list.append(w[0].lower()) #add token to the text list for the current line.\n",
    "      pos_list.append(w[1]) #add pos tag to the pos list for the current line.\n",
    "    dataset.append((text_list, pos_list)) #add the processed line (text and pos list) into the dataset list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lps3RqC3FAgA"
   },
   "source": [
    "## 3. Split the loaded dataset into training/dev/testing subsets\n",
    "\n",
    "Here we follow the common scheme to split this dataset into training/dev/test set with ratio 80%-10%-10%. After shuffling the data to avoid any possible ordering bias, we take the first 80% samples as training set, then the next 10% samples as dev set, and the last 10% samples as testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R6Mdvr5WCfZt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(436)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "training_data = dataset[0:math.floor(len(dataset)*0.8)]\n",
    "dev_data = dataset[math.floor(len(dataset)*0.8):math.floor(len(dataset)*0.9)]\n",
    "test_data = dataset[math.floor(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WzNS6VagmvT"
   },
   "source": [
    "## 4. Construct word-to-index and tag-to-index mapping dictionary\n",
    "\n",
    "We model POS tagging  as a sequence labeling task. The pipeline can be described as:\n",
    "\n",
    "<br>\n",
    "\n",
    "*a sequence of tokens -(mapping)-> a sequence of token indices --(input)--> POS Tagger --(output)--> a sequence of POS tag indices --(mapping)--> a sequence of POS tags*\n",
    "\n",
    "<br>\n",
    "\n",
    "So in this step, we want to construct two mapping dictionaries to assign a unique index to each token and tag respectively (see code below). These two mapping dictionaries will be used in the first and last mapping steps in the above pipeline.\n",
    "\n",
    "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "As in Part1, we also need to deal with **unknown tokens** in testing. The code below is implementing a possible way to adderess this problem. ***Q5: Please add comments to the lines with \"COMMENT NEEDED\". And describe in your words the implemented solution.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wg0oO1B4OlsD"
   },
   "outputs": [],
   "source": [
    "class Word_Dictionary(object): #this class is used to generate and return the word-to-index (index-to-word) vocabulary dictionary.\n",
    "    def __init__(self): #initializing.\n",
    "        self.word2idx = {'_unk_':0} # \"COMMENT NEEDED\" #a word for unknown tokens is added to the dictionary word2idx\n",
    "        self.idx2word = {0:'_unk_'} # \"COMMENT NEEDED\" #a word for unknown tokens is added to the dictionary idx2word\n",
    "        self.idx = 1\n",
    "    \n",
    "    def add_word(self, word): #add a new word into the dictionary and assign an unique index to it.\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __len__(self): #return the number of words in this dictionary.\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Tag_Dictionary(object): #this class is used to generate and return the tag-to-index (index-to-tag) dictionary.\n",
    "    def __init__(self): #initializing.\n",
    "        self.tag2idx = {}\n",
    "        self.idx2tag = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_tag(self, tag): #add a new tag into the dictionary and assign an unique index to it.\n",
    "        if not tag in self.tag2idx:\n",
    "            self.tag2idx[tag] = self.idx\n",
    "            self.idx2tag[self.idx] = tag\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __len__(self): #return the number of tags in this dictionary.\n",
    "        return len(self.tag2idx)\n",
    "\n",
    "\n",
    "wd_dict = Word_Dictionary() #initialize the word-to-index dictionary.\n",
    "tag_dict = Tag_Dictionary() #initialize the tag-to-index dictionary.\n",
    "unknown_threshold = 1 # \"COMMENT NEEDED\" # set frequency threshold for a word to be included in the dictionary.\n",
    "\n",
    "word_count = {} #initialize the dictionary to save frequencies of words.\n",
    "\n",
    "for sample in training_data: #fill the word frequency dictionary with training data.\n",
    "    text, tags = sample\n",
    "    for word in text:\n",
    "      if word not in word_count.keys():\n",
    "        word_count[word] = 1\n",
    "      else:\n",
    "        word_count[word] += 1\n",
    "\n",
    "for sample in training_data: #fill the word-to-index and tag-to-index dictionary with training data.\n",
    "    text, tags = sample\n",
    "    for word in text:\n",
    "      if word_count[word] > unknown_threshold:  #\"COMMENT NEEDED\"# only include the words with frequencies over a certain threshold.\n",
    "        wd_dict.add_word(word)\n",
    "    for tag in tags:\n",
    "      tag_dict.add_tag(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8K7BDZ-rkra"
   },
   "source": [
    "## 5. Implement the function for data processing\n",
    "\n",
    "In this step, we implement a function processing our data into the format which is ready for neural POS tagger's training and testing.\n",
    "\n",
    "More specifically, we convert the input token and pos tag list into tensors (i.e., vectors) containing their corresponding indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6NYGuy5WCiHh"
   },
   "outputs": [],
   "source": [
    "def data_processing_for_lstm(sample, word_dict, tag_dict, unknown_threshold): #this function convert a sample into the format ready for our neural POS tagger.\n",
    "    text, tags = sample\n",
    "    word_ids = []; tag_ids = []\n",
    "\n",
    "    for word in text:\n",
    "      if word in word_dict.word2idx.keys() and word_count[word] > unknown_threshold: #map the token to its index if its frequency is over a threshold.\n",
    "        word_ids.append(word_dict.word2idx[word])\n",
    "      else: #map the token to the index of unknwon token if its frequency is not over a threshold.\n",
    "        word_ids.append(word_dict.word2idx['_unk_'])\n",
    "\n",
    "    for tag in tags: #map pos tags into indices using the tag-to-index dictionary.\n",
    "      tag_ids.append(tag_dict.tag2idx[tag])\n",
    "\n",
    "    word_ids = torch.from_numpy(np.array(word_ids)) #convert the list of token ids into tensor format.\n",
    "    tag_ids = torch.from_numpy(np.array(tag_ids)) #convert the list of tag ids into tensor format.\n",
    "\n",
    "    return word_ids, tag_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHAotm7rtTg"
   },
   "source": [
    "## 6. The class of LSTM-based POS tagger\n",
    "\n",
    "Now let's design the architecture of our bidirectional stacked LSTM-based POS tagger. It should consist of three layers:\n",
    "\n",
    "* [Embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html): projecting input token ids into its embedding space.\n",
    "* [(Bi-)LSTM hidden state layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "* [Output layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): converting the hidden states to POS predictions.\n",
    "\n",
    "**Q6:** Please after carefully reading the pytorch links above, ***add comments to the lines with \"COMMENT NEEDED\".***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DP60pf4ByqPs"
   },
   "outputs": [],
   "source": [
    "def zero_state(module, batch_size, device): #the function to initialize the states of the bidirectionsl LSTM.\n",
    "    # * 2 is for the two directions\n",
    "    return Variable(torch.zeros(module.num_layers * 2, batch_size, module.hidden)).to(device), \\\n",
    "           Variable(torch.zeros(module.num_layers * 2, batch_size, module.hidden)).to(device)\n",
    "\n",
    "class LSTM_tagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden, num_layers, tag_size, device):\n",
    "        super(LSTM_tagger, self).__init__()\n",
    "        self.device = device #the device we run our model on.\n",
    "        self.num_layers = num_layers #ADD COMMENT # number of layers of LSTM.\n",
    "        self.hidden = hidden #the dimension of hidden state.\n",
    "        self.input_size = embed_size #ADD COMMENT #the size of word embedding (input of LSTM).\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) #the word embedding layer (convert words to embeddings).\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=0, #ADD COMMENT\n",
    "                            bidirectional=True) #ADD COMMENT #the LSTM layer is bidirectional.\n",
    "        self.h2s = nn.Linear(hidden * 2, tag_size) #the linear output layer (convert from hidden states of LSTM to the list of tag scores).\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) #convert word list to embedding list for the input sample.\n",
    "        x = x.view(len(x),1,-1)\n",
    "        s = zero_state(self, 1, self.device) #initialize the states of LSTM.\n",
    "        lstm_output, _ = self.lstm(x, s) #LSTM.\n",
    "        outputs = self.h2s(lstm_output.view(len(x),-1)) #ADD COMMENT #convert the output of LSTM into the prediction (sequence of tag score lists).\n",
    "        tags_probs = F.log_softmax(outputs,dim=1) #ADD COMMENT #for each score list, apply softmax to turn those into probabilities.\n",
    "\n",
    "        return tags_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQwufkRpr432"
   },
   "source": [
    "## 7. Initialize model and define parameters\n",
    "\n",
    "**ONLY IN SOLUTION =====**\n",
    "The parameters we need to initialize include:\n",
    "\n",
    "* embed_size: the dimension of token embeddings.\n",
    "* intermediate_size: the dimension of the hidden state of LSTM.\n",
    "* num_layers: the layer number of LSTM.\n",
    "* vocab_size: the size of vocabulary (in the word-to-index mapping dictionary).\n",
    "* tag_size: the number of POS tags (in the tag-to-index mapping dictionary).\n",
    "* num_epochs: how many times we want our model to be trained on training set.\n",
    "* learning rate.\n",
    "* device: the no. of GPU you want our model to run on.\n",
    "* criterion: the loss function, here we choose [cross entropy loss](https://pytorch.org/docs/1.9.1/generated/torch.nn.CrossEntropyLoss.html).\n",
    "* optimizer: here we use adam to train the model.\n",
    "\n",
    "The models we need to intialize is:\n",
    "\n",
    "* model: the one we implemented above.\n",
    "**=====**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1zpe1I2clIkV"
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "intermediate_size = 256\n",
    "num_layers = 2\n",
    "vocab_size = wd_dict.__len__()\n",
    "tag_size = tag_dict.__len__()\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.002\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.CrossEntropyLoss() #ADD COMMENT #the loss function, here we choose cross entropy loss.\n",
    "\n",
    "model = LSTM_tagger(vocab_size, embed_size, intermediate_size, num_layers, tag_size, device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #ADD COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN2ZI8wEr90x"
   },
   "source": [
    "## 8. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLkBijlBlJTg",
    "outputId": "d3e03235-56e5-4a1a-ddc5-6e9e24e54035"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1776/1776 [02:08<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4701, F1 score:  0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████████████▍                                               | 906/1776 [01:05<01:02, 13.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#backward and optimize.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/school/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/school/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    avg_ppl = 0; avg_loss = 0;\n",
    "    for i in tqdm(range(0, len(training_data))): #input one sample a time to train the model.\n",
    "        inputs, targets = data_processing_for_lstm(training_data[i], wd_dict, tag_dict, unknown_threshold)  #prepare the sample for model training.\n",
    "        \n",
    "        #forward pass.\n",
    "        outputs = model(inputs.to(device)) #ADD COMMENT #get prediction.\n",
    "        loss = criterion(outputs, targets.reshape(-1).to(device)) #ADD COMMENT #compute the loss between prediction and ground truth.\n",
    "        avg_loss += loss.item() #ADD COMMENT #add up the loss we get so far.\n",
    "        \n",
    "        #backward and optimize.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "    #validation step.\n",
    "    preds = []; targets = [];\n",
    "    for i in tqdm(range(0, len(dev_data))):\n",
    "        dev_inputs, dev_targets = data_processing_for_lstm(dev_data[i], wd_dict, tag_dict, unknown_threshold)\n",
    "        dev_outputs = model(dev_inputs.to(device))\n",
    "        dev_pred = torch.argmax(dev_outputs, dim=-1)\n",
    "        preds += dev_pred.detach().cpu().numpy().tolist()\n",
    "        targets += dev_targets.tolist()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, F1 score: {:5.2f}'\n",
    "        .format(epoch+1, num_epochs, avg_loss/len(training_data), f1_score(targets, preds, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXCXOQx9sFb-"
   },
   "source": [
    "## 9. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGBnD9cuSxgT",
    "outputId": "d6643004-3dd0-4089-db11-8775cbe44a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9193238354070685\n"
     ]
    }
   ],
   "source": [
    "test_preds = []; test_targets = [];\n",
    "for i in tqdm(range(0, len(test_data))):\n",
    "    test_input, test_target = data_processing_for_lstm(test_data[i], wd_dict, tag_dict, unknown_threshold)\n",
    "    test_output = model(test_input.to(device))\n",
    "    test_pred = torch.argmax(test_output, dim=-1)\n",
    "    test_preds += test_pred.detach().cpu().numpy().tolist()\n",
    "    test_targets += test_target.tolist()\n",
    "print(f1_score(test_targets, test_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37a9aRkBaKvq"
   },
   "source": [
    "**Q7:** Now train and test a different model with a dropout rate of 0.05. Report the performance and a plausible explanation for the different performance of the two tested models (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi9Ai65Qd7aI"
   },
   "source": [
    "THE FOLLOWING WILL NOT BE PART OF THIS ASSIGMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXaUIOAivNP-",
    "outputId": "918e0743-bce7-43ef-e28e-c2685473c841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted indices for tags: \n",
      " tensor([31, 25, 21,  2,  4,  1,  9, 13], device='cuda:0')\n",
      "Word:there , Predicted Tag : 31 ==> EX \n",
      "Word:are , Predicted Tag : 25 ==> VBP \n",
      "Word:also , Predicted Tag : 21 ==> RB \n",
      "Word:plant , Predicted Tag : 2 ==> NN \n",
      "Word:and , Predicted Tag : 4 ==> CC \n",
      "Word:local , Predicted Tag : 1 ==> JJ \n",
      "Word:shops , Predicted Tag : 9 ==> NNS \n",
      "Word:. , Predicted Tag : 13 ==> . \n"
     ]
    }
   ],
   "source": [
    "test_sentence = (['there', 'are', 'also', 'plant', 'and', 'local', 'shops', '.'], ['NN', 'VBZ', 'NN','NN', 'VBZ', 'NN','NN'])\n",
    "\n",
    "# see what the scores are after training\n",
    "inputs, targets = data_processing_for_lstm(test_sentence, wd_dict, tag_dict, unknown_threshold)\n",
    "tag_scores = model(inputs.to(device))\n",
    "\n",
    "# print the most likely tag index, by grabbing the index with the maximum score!\n",
    "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "\n",
    "print('The predicted indices for tags: \\n',predicted_tags)\n",
    "for id, word in enumerate(test_sentence[0]):\n",
    "  print('Word:{} , Predicted Tag : {} ==> {} '.format(word,predicted_tags[id], tag_dict.idx2tag[predicted_tags[id].item()]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SOLUTIONS-Assignment 4 (part 2): Neural_POS_Tagging_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
