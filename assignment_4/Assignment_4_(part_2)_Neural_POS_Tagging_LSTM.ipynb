{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyG4lTR4D7Lg"
   },
   "source": [
    "#  Assignment 4 (part 2)(CPSC 436N): LSTM-Based Part-Of-Speech (POS) Tagger\n",
    "\n",
    "In Part2 of this assignment, we are also using the text file called __cmpt-hw2-3.txt__ to train a bidirectional stacked LSTM-based neural sequence labeling model to predict the part of speech tags of unknown input sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFaEQAQgEIew"
   },
   "source": [
    "## 1. import libs and packages needed for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cti1kgGqD9a4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEzE9RjwE0fI"
   },
   "source": [
    "## 2. Load the dataset (cmpt-hw2-3.txt)\n",
    "\n",
    "We will use the same dataset as in (Part1), namely **cmpt-hw2-3.txt**. Please first upload this text file from your local computer to Colab by following these instructions:\n",
    "\n",
    "https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "**Please note that the uploaded file will be deleted once the connection is terminated. So you need to upload the file everytime you connect to google colab.**\n",
    "\n",
    "<br>\n",
    "\n",
    "For convenience, we will load and save each line of the dataset as **a token list** and **a pos list**. For example, one line in our dataset:\n",
    "\n",
    "*There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n",
    "\n",
    "will be saved as:\n",
    "\n",
    "__token list:__ ['there', 'are', 'also', 'plant', 'and', 'gift', 'shops', '.']\n",
    "\n",
    "__pos list:__ ['EX', 'VBP', 'RB', 'NN', 'CC', 'NN', 'NNS', '.']\n",
    "\n",
    "\n",
    "**Please note that we convert each token into its lower case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_ob5y-z7psi"
   },
   "outputs": [],
   "source": [
    "data_path = './cmpt-hw2-3.txt'\n",
    "\n",
    "dataset = [] #initialize dataset list.\n",
    "for line in open(data_path): #load data from the text file.\n",
    "    text_list = []; pos_list = [];\n",
    "    l = line.strip().split(' ') #split each line in the file by space.\n",
    "    for w in l:\n",
    "      w = w.split('_') #each token and its pos tag are connected by an underscore, here we split it by undescore.\n",
    "      text_list.append(w[0].lower()) #add token to the text list for the current line.\n",
    "      pos_list.append(w[1]) #add pos tag to the pos list for the current line.\n",
    "    dataset.append((text_list, pos_list)) #add the processed line (text and pos list) into the dataset list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lps3RqC3FAgA"
   },
   "source": [
    "## 3. Split the loaded dataset into training/dev/testing subsets\n",
    "\n",
    "Here we follow the common scheme to split this dataset into training/dev/test set with ratio 80%-10%-10%. After shuffling the data to avoid any possible ordering bias, we take the first 80% samples as training set, then the next 10% samples as dev set, and the last 10% samples as testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6Mdvr5WCfZt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "training_data = dataset[0:math.floor(len(dataset)*0.8)]\n",
    "dev_data = dataset[math.floor(len(dataset)*0.8):math.floor(len(dataset)*0.9)]\n",
    "test_data = dataset[math.floor(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WzNS6VagmvT"
   },
   "source": [
    "## 4. Construct word-to-index and tag-to-index mapping dictionary\n",
    "\n",
    "We model POS tagging  as a sequence labeling task. The pipeline can be described as:\n",
    "\n",
    "<br>\n",
    "\n",
    "*a sequence of tokens -(mapping)-> a sequence of token indices --(input)--> POS Tagger --(output)--> a sequence of POS tag indices --(mapping)--> a sequence of POS tags*\n",
    "\n",
    "<br>\n",
    "\n",
    "So in this step, we want to construct two mapping dictionaries to assign a unique index to each token and tag respectively (see code below). These two mapping dictionaries will be used in the first and last mapping steps in the above pipeline.\n",
    "\n",
    "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "As in Part1, we also need to deal with **unknown tokens** in testing. The code below is implementing a possible way to adderess this problem. ***Q5: Please add comments to the lines with \"COMMENT NEEDED\". And describe in your words the implemented solution.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg0oO1B4OlsD"
   },
   "outputs": [],
   "source": [
    "class Word_Dictionary(object): #this class is used to generate and return the word-to-index (index-to-word) vocabulary dictionary.\n",
    "    def __init__(self): #initializing.\n",
    "        self.word2idx = {'_unk_':0} # \"COMMENT NEEDED\" (Q5)\n",
    "        self.idx2word = {0:'_unk_'} # \"COMMENT NEEDED\" (Q5)\n",
    "        self.idx = 1\n",
    "    \n",
    "    def add_word(self, word): #add a new word into the dictionary and assign an unique index to it.\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __len__(self): #return the number of words in this dictionary.\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Tag_Dictionary(object): #this class is used to generate and return the tag-to-index (index-to-tag) dictionary.\n",
    "    def __init__(self): #initializing.\n",
    "        self.tag2idx = {}\n",
    "        self.idx2tag = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_tag(self, tag): #add a new tag into the dictionary and assign an unique index to it.\n",
    "        if not tag in self.tag2idx:\n",
    "            self.tag2idx[tag] = self.idx\n",
    "            self.idx2tag[self.idx] = tag\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __len__(self): #return the number of tags in this dictionary.\n",
    "        return len(self.tag2idx)\n",
    "\n",
    "\n",
    "wd_dict = Word_Dictionary() #initialize the word-to-index dictionary.\n",
    "tag_dict = Tag_Dictionary() #initialize the tag-to-index dictionary.\n",
    "unknown_threshold = 1 # \"COMMENT NEEDED\" (Q5)\n",
    "\n",
    "word_count = {} #initialize the dictionary to save frequencies of words.\n",
    "\n",
    "for sample in training_data: #fill the word frequency dictionary with training data.\n",
    "    text, tags = sample\n",
    "    for word in text:\n",
    "      if word not in word_count.keys():\n",
    "        word_count[word] = 1\n",
    "      else:\n",
    "        word_count[word] += 1\n",
    "\n",
    "for sample in training_data: #fill the word-to-index and tag-to-index dictionary with training data.\n",
    "    text, tags = sample\n",
    "    for word in text:\n",
    "      if word_count[word] > unknown_threshold:  #\"COMMENT NEEDED\" (Q5)\n",
    "        wd_dict.add_word(word)\n",
    "    for tag in tags:\n",
    "      tag_dict.add_tag(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8K7BDZ-rkra"
   },
   "source": [
    "## 5. Implement the function for data processing\n",
    "\n",
    "In this step, we implement a function processing our data into the format which is ready for neural POS tagger's training and testing.\n",
    "\n",
    "More specifically, we convert the input token and pos tag list into tensors (i.e., vectors) containing their corresponding indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NYGuy5WCiHh"
   },
   "outputs": [],
   "source": [
    "def data_processing_for_lstm(sample, word_dict, tag_dict, unknown_threshold): #this function convert a sample into the format ready for our neural POS tagger.\n",
    "    text, tags = sample\n",
    "    word_ids = []; tag_ids = []\n",
    "\n",
    "    for word in text:\n",
    "      if word in word_dict.word2idx.keys() and word_count[word] > unknown_threshold: #map the token to its index if its frequency is over a threshold.\n",
    "        word_ids.append(word_dict.word2idx[word])\n",
    "      else: #map the token to the index of unknwon token if its frequency is not over a threshold.\n",
    "        word_ids.append(word_dict.word2idx['_unk_'])\n",
    "\n",
    "    for tag in tags: #map pos tags into indices using the tag-to-index dictionary.\n",
    "      tag_ids.append(tag_dict.tag2idx[tag])\n",
    "\n",
    "    word_ids = torch.from_numpy(np.array(word_ids)) #convert the list of token ids into tensor format.\n",
    "    tag_ids = torch.from_numpy(np.array(tag_ids)) #convert the list of tag ids into tensor format.\n",
    "\n",
    "    return word_ids, tag_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHAotm7rtTg"
   },
   "source": [
    "## 6. The class of LSTM-based POS tagger\n",
    "\n",
    "Now let's design the architecture of our bidirectional stacked LSTM-based POS tagger. It should consist of three layers:\n",
    "\n",
    "* [Embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html): projecting input token ids into its embedding space.\n",
    "* [(Bi-)LSTM hidden state layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "* [Output layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): converting the hidden states to POS predictions.\n",
    "\n",
    "**Q6:** Please after carefully reading the pytorch links above, ***add comments to the lines with \"COMMENT NEEDED\".***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP60pf4ByqPs"
   },
   "outputs": [],
   "source": [
    "def zero_state(module, batch_size, device): #the function to initialize the states of the bidirectionsl LSTM.\n",
    "    # * 2 is for the two directions\n",
    "    return Variable(torch.zeros(module.num_layers * 2, batch_size, module.hidden)).to(device), \\\n",
    "           Variable(torch.zeros(module.num_layers * 2, batch_size, module.hidden)).to(device)\n",
    "\n",
    "class LSTM_tagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden, num_layers, tag_size, device):\n",
    "        super(LSTM_tagger, self).__init__()\n",
    "        self.device = device #the device we run our model on.\n",
    "        self.num_layers = num_layers #ADD COMMENT (Q6)\n",
    "        self.hidden = hidden #the dimension of hidden state.\n",
    "        self.input_size = embed_size #ADD COMMENT (Q6)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) #the word embedding layer (convert words to embeddings).\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=0, #ADD COMMENT (Q6)\n",
    "                            bidirectional=True) #ADD COMMENT (Q6)\n",
    "        self.h2s = nn.Linear(hidden * 2, tag_size) #the linear output layer (convert from hidden states of LSTM to the list of tag scores).\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) #convert word list to embedding list for the input sample.\n",
    "        x = x.view(len(x),1,-1)\n",
    "        s = zero_state(self, 1, self.device) #initialize the states of LSTM.\n",
    "        lstm_output, _ = self.lstm(x, s) #LSTM.\n",
    "        outputs = self.h2s(lstm_output.view(len(x),-1)) #ADD COMMENT (Q6)\n",
    "        tags_probs = F.log_softmax(outputs,dim=1) #ADD COMMENT (Q6)\n",
    "\n",
    "        return tags_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQwufkRpr432"
   },
   "source": [
    "## 7. Initialize model and define parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zpe1I2clIkV"
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "intermediate_size = 256\n",
    "num_layers = 2\n",
    "vocab_size = wd_dict.__len__()\n",
    "tag_size = tag_dict.__len__()\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.002\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' # Specify the device (CPU/GPU) to train/eval the model on\n",
    "criterion = nn.CrossEntropyLoss() #ADD COMMENT \n",
    "\n",
    "model = LSTM_tagger(vocab_size, embed_size, intermediate_size, num_layers, tag_size, device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #ADD COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN2ZI8wEr90x"
   },
   "source": [
    "## 8. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLkBijlBlJTg",
    "outputId": "d3e03235-56e5-4a1a-ddc5-6e9e24e54035"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    avg_ppl = 0; avg_loss = 0;\n",
    "    for i in tqdm(range(0, len(training_data))): #input one sample a time to train the model.\n",
    "        inputs, targets = data_processing_for_lstm(training_data[i], wd_dict, tag_dict, unknown_threshold)  #prepare the sample for model training.\n",
    "        \n",
    "        #forward pass.\n",
    "        outputs = model(inputs.to(device)) #ADD COMMENT \n",
    "        loss = criterion(outputs, targets.reshape(-1).to(device)) #ADD COMMENT \n",
    "        avg_loss += loss.item() #ADD COMMENT \n",
    "        \n",
    "        #backward and optimize.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "    #validation step.\n",
    "    preds = []; targets = [];\n",
    "    for i in range(0, len(dev_data)):\n",
    "        dev_inputs, dev_targets = data_processing_for_lstm(dev_data[i], wd_dict, tag_dict, unknown_threshold)\n",
    "        dev_outputs = model(dev_inputs.to(device))\n",
    "        dev_pred = torch.argmax(dev_outputs, dim=-1)\n",
    "        preds += dev_pred.detach().cpu().numpy().tolist()\n",
    "        targets += dev_targets.tolist()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, F1 score: {:5.2f}'\n",
    "        .format(epoch+1, num_epochs, avg_loss/len(training_data), f1_score(targets, preds, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXCXOQx9sFb-"
   },
   "source": [
    "## 9. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGBnD9cuSxgT",
    "outputId": "d6643004-3dd0-4089-db11-8775cbe44a8d"
   },
   "outputs": [],
   "source": [
    "test_preds = []; test_targets = [];\n",
    "for i in tqdm(range(0, len(test_data))):\n",
    "    test_input, test_target = data_processing_for_lstm(test_data[i], wd_dict, tag_dict, unknown_threshold)\n",
    "    test_output = model(test_input.to(device))\n",
    "    test_pred = torch.argmax(test_output, dim=-1)\n",
    "    test_preds += test_pred.detach().cpu().numpy().tolist()\n",
    "    test_targets += test_target.tolist()\n",
    "print(f1_score(test_targets, test_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37a9aRkBaKvq"
   },
   "source": [
    "**Q7:** Now train and test a different model with a dropout rate of 0.05. Report the performance and a plausible explanation for the different performance of the two tested models (if any)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Assignment 4 (part 2): Neural_POS_Tagging_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
