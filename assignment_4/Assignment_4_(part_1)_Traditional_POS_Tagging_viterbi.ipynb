{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESA2kDcMmNkJ"
   },
   "source": [
    "##Assignment 4 (part 1)(CPSC 436N): Count-based Sequence labeling  with Viterbi Algorithm\n",
    "\n",
    "In Part1 of this assignment, we are going to use the the text file called __cmpt-hw2-3.txt__ to build a Hidden Markov Model to predict the part of speech tags for the words in a given sentence using the *Viterbi Algorithm* (see textbook sec 8.4.5). In class we learned about the *Forward Algorithm* and Viterbi is a slight variation of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AD7kg1fkklb"
   },
   "source": [
    "## 1. Loading the dataset (cmpt-hw2-3.txt)\n",
    "\n",
    "In order to ease later steps, we can load and save the cmpt-hw2-3.txt dataset (i.e., our corpus annotated with POSs) in a format different from its origianl one. In the cmpt-hw2-3.txt each word is followed by an underscore and a tag that represents the wordâ€™s correct part of speech in that context. For instance:\n",
    "\n",
    "<br>\n",
    "\n",
    "*(1) There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n",
    "\n",
    "*(2) Tosco_NNP said_VBD it_PRP expects_VBZ BP_NNP to_TO shut_VB the_DT plant_NN ._.*\n",
    "\n",
    "<br>\n",
    "\n",
    "We will instead reformat and load the same info as (token, pos_tag) lists. For example:\n",
    "\n",
    "<br>\n",
    "\n",
    "[\n",
    "  [(there, EX), (are, VBP), (also, RB), (plant, NN), (and, CC), (gift, NN), (shop, NNS), (., .)],\n",
    "\n",
    "  [(tosco, NNP), (said, VBD), (it, PRP), (expects, VBZ), (BP, NNP), (to, To), (shut, VB), (the, DT), (plant, NN), (., .)]\n",
    "]\n",
    "\n",
    "<br>\n",
    "\n",
    "**Please note that we convert each token into its lower case.**\n",
    "\n",
    "Also in this step, we  obtain the set of states for the HHM (the POSs)  and set of observation (the tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PmreO0EUmLm-"
   },
   "outputs": [],
   "source": [
    "data_path = 'cmpt-hw2-3.txt'\n",
    "\n",
    "dataset = []; states = []; observations = []; #initialize dataset, state and observation list.\n",
    "\n",
    "for line in open(data_path): #load data from the text file.\n",
    "  text_pos_list = [];\n",
    "  l = line.strip().split(' ') #split each line in the file by space.\n",
    "  for w in l:\n",
    "    w = w.split('_') #each token and its pos tag are connected by an underscore, here we split it by underscore.\n",
    "    states.append(w[1]) #add pos tag to the initial state list.\n",
    "    observations.append(w[0].lower()) #lowercase the token and add it to the initial observation list.\n",
    "    text_pos_list.append((w[0].lower(), w[1])) #add the (token, pos) pair into the list for the sentence.\n",
    "  dataset.append(text_pos_list) #add the processed sentence list into the dataset list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5phXfOnGueR_"
   },
   "source": [
    "## 2. Further processing the dataset, state and observation list\n",
    "\n",
    "In this step, we need to consider two issue:\n",
    "\n",
    "**Q1(a)** What is the problem with the state lists we just obtained? report the problem and fix it in the code below.\n",
    "\n",
    "\n",
    "**Q1(b)** The first pass over the dataset generates a fixed list of vocabulary tokens. What would happen if the input sentence we want to tag contained unknown tokens, i.e which are not covered in the vocabulary list? \n",
    "The code below is implementing a possible way to adderess this problem. Please add comments to the lines with \"COMMENT NEEDED\". And describe in your words the implemented solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW-YJkjcrr7R"
   },
   "outputs": [],
   "source": [
    "#SOLUTION Q1(a)\n",
    "\n",
    "#*************\n",
    "\n",
    "unk_list = []; #COMMENT NEEDED Q1(b)\n",
    "\n",
    "for o in tuple(set(observations)):\n",
    "  if observations.count(o) < 2:\n",
    "     unk_list.append(o) #COMMENT NEEDED Q1(b)\n",
    "\n",
    "for i in range(len(observations)):\n",
    "  if observations[i] in unk_list:\n",
    "     observations[i] = '_unk_' #COMMENT NEEDED Q1(b)\n",
    "\n",
    "observations = tuple(set(observations)) #COMMENT NEEDED Q1(b)\n",
    "\n",
    "for text_pos_i in range(len(dataset)): #COMMENT NEEDED Q1(b)\n",
    "  for w_i in range(len(dataset[text_pos_i])):\n",
    "    if dataset[text_pos_i][w_i][0] in unk_list:\n",
    "       dataset[text_pos_i][w_i] = ('_unk_', dataset[text_pos_i][w_i][1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBqKKgjJzWCT"
   },
   "source": [
    "## 3. Computing the Emission Probabilities\n",
    "\n",
    "The emission probability matrix can be formed as a dictionary, **Q2:** please try to understand the structure of this dictionary and complete the following code to obtain the emission probability matrix with a final normalization+smoothing step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR8sPEVUxwHm"
   },
   "outputs": [],
   "source": [
    "def normalize_and_smoothing(d, all_elements):\n",
    "   for e in all_elements:\n",
    "     if e not in d.keys():\n",
    "       d[e] = 0\n",
    "   raw = sum(d.values())+len(d)\n",
    "   factor = float(1)/raw\n",
    "   output = {key:(value+1)*factor for key,value in d.items()}\n",
    "   return output\n",
    "\n",
    "emission_probability = {} #initialize the emission probability matrix.\n",
    "for sent in dataset: #traverse the dataset to obtain the emission probability matrix.\n",
    "  for i in range(len(sent)):\n",
    "    if sent[i][1] not in emission_probability.keys(): #if the state (pos) is not in the key list of emission probability dict yet, intiailize a subdict for it and add the corresponding observation (token).\n",
    "       emission_probability[sent[i][1]] = {}\n",
    "       emission_probability[sent[i][1]][sent[i][0]] = 1\n",
    "    else: #if the state (pos) is in the key list of emission probability dict, update the frequency of the corresponding observation (token).\n",
    "      if sent[i][0] not in emission_probability[sent[i][1]].keys():\n",
    "         emission_probability[sent[i][1]][sent[i][0]] = 1\n",
    "      else:\n",
    "         emission_probability[sent[i][1]][sent[i][0]] += 1\n",
    "            \n",
    "#****** SOLUTION Q2 *******\n",
    "\n",
    "#*************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrgHDEf23WOO"
   },
   "source": [
    "## 4. Computing the Start and Transition Probabilities\n",
    "\n",
    "The Transition and Strat probability matrices can be similarly formed as dictionaries,** Q2 cont'** please implement the normalization function also for the transition probabilitie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRvHUphUz2G7"
   },
   "outputs": [],
   "source": [
    "transition_probability = {} #initialize the transition probability matrix.\n",
    "\n",
    "for sent in dataset: #traverse the dataset to obtain the transition probability matrix.\n",
    "  for i in range(len(sent)-1):\n",
    "    if sent[i][1] not in transition_probability.keys(): #if the current state (pos) is not in the key list of transition probability dict yet, intiailize a subdict for it and add its subsequent state.\n",
    "       transition_probability[sent[i][1]] = {}\n",
    "       transition_probability[sent[i][1]][sent[i+1][1]] = 1\n",
    "    else: #if the state (pos) is in the key list of transition probability dict, update the frequency of its subsequent state.\n",
    "      if sent[i+1][1] not in transition_probability[sent[i][1]].keys():\n",
    "         transition_probability[sent[i][1]][sent[i+1][1]] = 1\n",
    "      else:\n",
    "         transition_probability[sent[i][1]][sent[i+1][1]] += 1\n",
    "\n",
    "#********* SOLUTION Q2 *******\n",
    "\n",
    "\n",
    "#*******************************\n",
    "\n",
    "\n",
    "start_probability = {} #initialize the transition probability matrix.\n",
    "for sent in dataset: #construct the start probability dictionary which contains the frequency of each state appearing at the start position of a sentence.\n",
    "  if sent[0][1] not in start_probability.keys():\n",
    "     start_probability[sent[0][1]] = 1\n",
    "  else:\n",
    "     start_probability[sent[0][1]] += 1\n",
    "start_probability = normalize_and_smoothing(start_probability, states) #normalize the start probability dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBW-HYEXjfPm"
   },
   "source": [
    "## 5. Implementing the Viterbi Algorithm \n",
    "\n",
    "The code below is implementing the Viterbi algorithm. <br>\n",
    "**Q3**: Please add comments to the lines with \"COMMENT NEEDED\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjjacMrG34Hb"
   },
   "outputs": [],
   "source": [
    "def Viterbi(input_observations, states, start_probability, transition_probability, emission_probability):\n",
    "    path = {s:[] for s in states} #this dictionary saves the shortest path for each state.\n",
    "    curr_probability = {}\n",
    "    for s in states:\n",
    "        curr_probability[s] = start_probability[s]*emission_probability[s][input_observations[0]] #\"COMMENT NEEDED\" (Q3)\n",
    "    for i in range(1, len(input_observations)):\n",
    "        last_probability = curr_probability #update the current state probabilities to the ones obtained in the last step.\n",
    "        curr_probability = {} #initialize a new dict to save the state probabilities for this step. \n",
    "        for curr_state in states: #for each possible state in this step, compute all possible paths and their probabilities towards this state.\n",
    "            possible_paths = [(last_probability[last_state]*transition_probability[last_state][curr_state]*emission_probability[curr_state][input_observations[i]], last_state) for last_state in states]\n",
    "            max_probability, last_state = max(possible_paths) #\"COMMENT NEEDED\" (Q3)\n",
    "            curr_probability[curr_state] = max_probability #update the current state probabilities.\n",
    "            path[curr_state].append(last_state) #record the optimal path towards each possible state in this step.\n",
    "    for s in states:\n",
    "        path[s].append(s) #complete the path with each state as the end.\n",
    "\n",
    "   #\"COMMENT NEEDED\" (Q3)\n",
    "    sorted_probability = sorted(curr_probability.items(), key=lambda x: x[1], reverse=True)\n",
    "    the_last_state = sorted_probability[0][0]\n",
    "    optimal_path = path[the_last_state]\n",
    "\n",
    "    return optimal_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyrz5CKi_QAr"
   },
   "source": [
    "## 6. Applying the the Viterbi Algorithm \n",
    "\n",
    "**Q4:** Apply the Viterbi Algorithm to the following sentences (remember to deal with unknown words): <br>\n",
    "(a) \"there are also many plants in the square.\" <br>\n",
    "(b) \"we will move the table into cs123.\" <br>\n",
    "\n",
    "Please report the tagging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0ESw9woAXzk"
   },
   "outputs": [],
   "source": [
    "# SOLUTION Q4(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ0_L4X7Au6k"
   },
   "outputs": [],
   "source": [
    "# SOLUTION Q4(b)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4 (part 1): Traditional_POS_Tagging_viterbi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
